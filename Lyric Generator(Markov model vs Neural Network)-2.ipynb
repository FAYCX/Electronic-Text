{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5188ea67-381e-4710-a434-d96f07c1f52e",
   "metadata": {},
   "source": [
    "# Lyric Generator: Comparing RNN and Markov Models\n",
    "### Assignment 5\n",
    "\n",
    "In this week's NLP lab, I built two different Michael Jackson style Lyric Generators using Markov Chain and Neural Network methodologies. My goal is to explore the differences between a lexicon-based model, such as the Markov chain, and deep learning models like Neural Networks. Through the process, it is hoped that I could better understand their respective strengths and weaknesses and investigate their potential applications in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066cbae-de7b-46ae-90d1-574fff280e29",
   "metadata": {},
   "source": [
    "## Final Outcome\n",
    "\n",
    "Throughout this week's exploration, I encountered several challenges. Initially, collecting my database for this project was straightforward, as free lyrics websites are ubiquitous on the internet. However, I faced some trouble encoding the system for raw string text. I learned to use the .fit_on_texts function to tokenize the individual lexicon in the dataset and build a preliminary vocabulary system. Due to the time constraint, I didn't invest much effort in data cleaning and feature engineering — two steps I recognize as crucial for any industry usage project — before proceeding with my lab journey.\n",
    "\n",
    "For the LSTM Recurrent Network Model, I followed a simple tutorial from a machine learning course I'm currently taking. After tweaking and adjusting different parameters and the logic within the model's structure, I commenced the training process. Initially, to prevent overfitting, I used an `early_stop` function, but was quite disappointed with the resulting accuracy of 0.3. I then commented out that part, retrained the model for 100 epochs, and achieved a new accuracy of 0.56. Although the final result isn't ideal compared to existing NLP language models, the main purpose of this journey was to explore the possibilities of domain-specific model training processes. I believe with a larger and better-structured dataset and more sophisticated procedures, the results could be significantly improved.\n",
    "\n",
    "Compared to the RNN model, in this case particular, I found that the Markov Chain Model requires less data to produce reasonable outputs and is very time-efficient! For instance, training just one RNN model with a 250KB dataset took me over an hour to complete on the latest MacBook Pro, while training two Markov models took less than a minute altogether. And for this project's tasks, where the linguistic structure is relatively simple and the output might relies more on the beauty of randomness rather than its deep coherence, Markov chains performed slightly better than RNNs, which was an interesting outcome to me.\n",
    "\n",
    "Below are some final output from different models, as well as this week's lab process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a13a540-eed9-4960-99fb-a650215445e3",
   "metadata": {},
   "source": [
    "###  RNN Model\n",
    "                                         \n",
    "    \n",
    "    I had a dream of another tomorrow of life                                 \n",
    "    and night i'll find day years may side right \n",
    "    just ridiculed or fast falls land though falls      \n",
    "    real son wings him chiller gana plan pure cola \n",
    "    hollywood across remedy high nigh high nigh high nigh high \n",
    "    chiller gloom stalking mine      \n",
    "    baby tomorrow other's nowhere tambien\n",
    "\n",
    "--------\n",
    "    \n",
    "    Spring here with you\n",
    "    but it feel\n",
    "    i was around\n",
    "    yeah yeah yeah\n",
    "    yeah yeah yeah\n",
    "    yeah yeah here\n",
    "    worried feed eliminate\n",
    "    leaving evening moonwalk\n",
    "    millionaire it’s deal\n",
    "    or estas alright\n",
    "    oak tree tops\n",
    "    funny top dear\n",
    "    lovely boat cruise\n",
    "    heaven\n",
    "\n",
    "---------\n",
    "\n",
    "    World i still remember the face\n",
    "    her world a kiss you\n",
    "    found your side so cold\n",
    "    so cold cold fin oak\n",
    "    tree tops que je yourself\n",
    "    addition by myself strong in\n",
    "    stalking singing to rise above\n",
    "    tomorrow stalking him selfish terrifying\n",
    "    theme pushed right revenge somehow\n",
    "    actual plan fin l bien\n",
    "    de bois tristeza ms inside\n",
    "    c doo doo oooooooh gana"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac72b100-a9fe-4410-8a2a-42f46efac2c0",
   "metadata": {},
   "source": [
    "###  Markov Model\n",
    "\n",
    "    \n",
    "    miss  me  feel  the  sun\n",
    "    sun  shine  i  apart  you're  my\n",
    "    darling,  now  just  like  the\n",
    "    blood  in  the  heart  lay\n",
    "    down  if  you  dig  it?\n",
    "    blood  in  the  heart  lay\n",
    "    lay  down  if  you  dig  it?\n",
    "    girl  just  because  we  can\n",
    "    close  as  one  and  shivers\n",
    "    in  time)  and  you  wanna\n",
    "    close  as  one  and  shivers\n",
    "    shivers  in  time)  and  you  wanna\n",
    "    say  is  cheap  you're  dirty\n",
    "    diana,  nah  dirty  diana,  nah (ooooh . . .  .)\n",
    "    diana,  nah  dirty  diana,  nah  nah  (ooooh . . .  .)\n",
    "    just  not  around  what  you\n",
    "    try  so  we  may  seem\n",
    "    to  be  recognized  the  money\n",
    "    try  so  we  may  seem\n",
    "    seem  to  be  recognized  the  money\n",
    "    throw  your  name  and  me\n",
    "    somehow  though  years  the  floor\n",
    "    in  me  (hold  on)  ain't\n",
    "    somehow  though  years  the  floor\n",
    "    floor  in  me  (hold  on)  ain't\n",
    "    nothin'  that  i  was  waiting\n",
    "    for  me  alone  whisper  three\n",
    "    to  be  startin'  somethin'  about\n",
    "    for  me  alone  whisper  three\n",
    "    three  to  be  startin'  somethin'  about\n",
    "\n",
    "-----\n",
    "\n",
    "    spring every year, yours and a child's heart\n",
    "    do me game don't stop pressurin' me here\n",
    "    at night there and it to shake my\n",
    "    baby cryin' wolf ain't hard to... take you\n",
    "    baby (you really matters i've never ran away\n",
    "\n",
    "-----\n",
    "\n",
    "    why  do  baby  your  eyes\n",
    "    eyes  are  kissin'  me,  sue  me\n",
    "    how  it  for  us  all\n",
    "    the  change)  hoo!  \n",
    "    i  have a  torch  will  be  there,\n",
    "    the  change)  hoo!  i  have\n",
    "    have  a  torch  will  be  there,\n",
    "    you  know  you  said  yeah\n",
    "    shoo-hee  woh  woh  woh  hee!\n",
    "    shoo-hee  oooh  foolish  trickery  and\n",
    "    shoo-hee  woh  woh  woh  hee! hee!  shoo-hee  oooh  \n",
    "    foolish  trickery  and\n",
    "    how  lust  for  us  we\n",
    "    have  been  my  hand  \n",
    "    ('cause you  got  the  way  that\n",
    "    have  been  my  hand)  \n",
    "    'cause you  got  the  way  that\n",
    "    girl  at  yourself  and  i\n",
    "    am  the  lost  my  baby\n",
    "    (yeah,  yeah)  too  high  speed,\n",
    "    am  the  lost  my  baby\n",
    "    baby  (yeah,  yeah)  too  high  speed,\n",
    "    feedback,  dolby  release  two  places\n",
    "    at  the  devil  anything  anything\n",
    "    anything  anything  anything  for  me\n",
    "    at  the  devil  anything  anything\n",
    "    anything  anything  anything  anything  for  me\n",
    "    with  me!)  i'm  telling  you\n",
    "    make  it  (bad  bad  -\n",
    "    you  on  time  around  comes\n",
    "    make  it  (bad  bad  -\n",
    "    you  on  time  around  comes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce195e8b-469e-4b4e-951e-3995c4746ed2",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7586024e-5278-4082-8b1b-c70c349601bf",
   "metadata": {},
   "source": [
    "# Lab Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4455e5cf-3ffb-4486-8f52-3b61b2b6e994",
   "metadata": {},
   "source": [
    "## `Dataset Introduction`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e1d24-b435-4af9-9253-64efc5066588",
   "metadata": {},
   "source": [
    "When considering machine learning models, my initial thoughts regarding the process invariably center on dataset collection. This concept mirrors cognitive science principles, suggesting that the foundational data structure underpinning our brain's functionality is closely linked to the final computational output. Thus, the selection of data should be directly related to its domain and purpose, embodying the essence of the targeted task.\n",
    "\n",
    "For this project, lexicon choice and rhyme structure emerge as two prominent features that significantly influence the final output. Aimed at resonating with the younger generation, I embarked on data mining and research to identify the most suitable dataset. After thorough exploration, I opted for a dataset which contains text strings from the singer Michael Jackson, representing the most plausible and effective choice for this endeavor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "61434985-1ea9-45b6-b706-cbaede9afc37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import random\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff815c7b-589a-45ed-a401-0271ebb68eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "textdata = open(\"lyrics/michael-jackson.txt\").read()\n",
    "#textdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "76bb4813-08c8-4082-b415-9c4e644dacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = textdata.lower().split(\"\\n\")\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2c65f1-19cf-4155-bb48-5b0582a195ec",
   "metadata": {},
   "source": [
    "## `Build a simple tokenizer system in the raw dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52b61f6e-a25e-445f-ae1d-625d76adb6ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b261874-60e6-4fcf-b792-eef6192f938c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3274"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "628c541d-fdce-4410-8918-da4d269feda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the length of the corpus:11177, This is the length of the text data: 251022\n"
     ]
    }
   ],
   "source": [
    "print(f\"This is the length of the corpus:{len(corpus)}, This is the length of the text data: {len(textdata)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97ed87f1-1263-4519-bd7b-b3976500640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"{tokenizer.word_index}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b254ecfc-4d17-45fb-9ca4-2d7c883d8c39",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1755c5a-68fd-43ba-aa9f-c07c643d511c",
   "metadata": {},
   "source": [
    "# LAB 1 - RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3fd7f793-e486-4f49-9794-ed138e699595",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d18ddc22-ef5e-4041-bc46-40b7b572870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "\n",
    "for line in corpus:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "\n",
    "# create predictors and labels\n",
    "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "\n",
    "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "73c00dbc-e663-4e52-8f94-3bf44ba61ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 30ms/step - accuracy: 0.0971 - loss: 5.8498\n",
      "Epoch 2/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.3018 - loss: 3.8804\n",
      "Epoch 3/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.3803 - loss: 3.1646\n",
      "Epoch 4/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.3789 - loss: 3.3300\n",
      "Epoch 5/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.4382 - loss: 2.6535\n",
      "Epoch 6/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.4545 - loss: 2.5393\n",
      "Epoch 7/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.4597 - loss: 2.5052\n",
      "Epoch 8/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.4829 - loss: 2.2977\n",
      "Epoch 9/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.5007 - loss: 2.2133\n",
      "Epoch 10/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.5086 - loss: 2.1573\n",
      "Epoch 11/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.5103 - loss: 2.1243\n",
      "Epoch 12/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.5140 - loss: 2.1124\n",
      "Epoch 13/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.5224 - loss: 2.0694\n",
      "Epoch 14/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.5198 - loss: 2.0757\n",
      "Epoch 15/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.5396 - loss: 1.9700\n",
      "Epoch 16/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 31ms/step - accuracy: 0.5258 - loss: 2.0548\n",
      "Epoch 17/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.5274 - loss: 2.0056\n",
      "Epoch 18/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 30ms/step - accuracy: 0.5286 - loss: 2.0601\n",
      "Epoch 19/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.5354 - loss: 1.9884\n",
      "Epoch 20/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.5296 - loss: 2.0260\n",
      "Epoch 21/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 31ms/step - accuracy: 0.5370 - loss: 1.9893\n",
      "Epoch 22/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.5356 - loss: 1.9880\n",
      "Epoch 23/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 31ms/step - accuracy: 0.5439 - loss: 1.9408\n",
      "Epoch 24/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 29ms/step - accuracy: 0.5371 - loss: 1.9908\n",
      "Epoch 25/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5476 - loss: 1.9202\n",
      "Epoch 26/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5518 - loss: 1.9031\n",
      "Epoch 27/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5324 - loss: 1.9779\n",
      "Epoch 28/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5513 - loss: 1.9028\n",
      "Epoch 29/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5530 - loss: 1.8974\n",
      "Epoch 30/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5515 - loss: 1.9192\n",
      "Epoch 31/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5502 - loss: 1.9116\n",
      "Epoch 32/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5502 - loss: 1.9126\n",
      "Epoch 33/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5413 - loss: 1.9729\n",
      "Epoch 34/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5433 - loss: 1.9511\n",
      "Epoch 35/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5538 - loss: 1.8933\n",
      "Epoch 36/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5546 - loss: 1.9264\n",
      "Epoch 37/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5500 - loss: 1.9433\n",
      "Epoch 38/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5505 - loss: 1.8946\n",
      "Epoch 39/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5493 - loss: 1.9074\n",
      "Epoch 40/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5564 - loss: 1.8658\n",
      "Epoch 41/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5588 - loss: 1.8696\n",
      "Epoch 42/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5574 - loss: 1.8883\n",
      "Epoch 43/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5516 - loss: 1.9177\n",
      "Epoch 44/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5612 - loss: 1.8388\n",
      "Epoch 45/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5629 - loss: 1.8575\n",
      "Epoch 46/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5676 - loss: 1.8159\n",
      "Epoch 47/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5617 - loss: 1.8596\n",
      "Epoch 48/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step - accuracy: 0.5582 - loss: 1.8774\n",
      "Epoch 49/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5570 - loss: 1.8692\n",
      "Epoch 50/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5663 - loss: 1.8309\n",
      "Epoch 51/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5652 - loss: 1.8533\n",
      "Epoch 52/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5613 - loss: 1.8500\n",
      "Epoch 53/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5679 - loss: 1.8380\n",
      "Epoch 54/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5589 - loss: 1.8920\n",
      "Epoch 55/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5611 - loss: 1.8487\n",
      "Epoch 56/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5610 - loss: 1.8679\n",
      "Epoch 57/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5643 - loss: 1.8608\n",
      "Epoch 58/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5688 - loss: 1.8123\n",
      "Epoch 59/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5691 - loss: 1.8111\n",
      "Epoch 60/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5663 - loss: 1.8270\n",
      "Epoch 61/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5679 - loss: 1.8259\n",
      "Epoch 62/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5755 - loss: 1.7861\n",
      "Epoch 63/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5650 - loss: 1.8398\n",
      "Epoch 64/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5700 - loss: 1.8309\n",
      "Epoch 65/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5634 - loss: 1.8590\n",
      "Epoch 66/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5661 - loss: 1.8361\n",
      "Epoch 67/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5671 - loss: 1.8469\n",
      "Epoch 68/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5727 - loss: 1.8004\n",
      "Epoch 69/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5670 - loss: 1.8285\n",
      "Epoch 70/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5726 - loss: 1.8130\n",
      "Epoch 71/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5763 - loss: 1.8071\n",
      "Epoch 72/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5653 - loss: 1.8441\n",
      "Epoch 73/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5744 - loss: 1.8124\n",
      "Epoch 74/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5669 - loss: 1.8740\n",
      "Epoch 75/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5644 - loss: 1.8802\n",
      "Epoch 76/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step - accuracy: 0.5711 - loss: 1.8444\n",
      "Epoch 77/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 26ms/step - accuracy: 0.5654 - loss: 1.8777\n",
      "Epoch 78/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5644 - loss: 1.8662\n",
      "Epoch 79/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5655 - loss: 1.8873\n",
      "Epoch 80/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5726 - loss: 1.8573\n",
      "Epoch 81/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5730 - loss: 1.8695\n",
      "Epoch 82/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5801 - loss: 1.8108\n",
      "Epoch 83/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5727 - loss: 1.8578\n",
      "Epoch 84/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5644 - loss: 1.9166\n",
      "Epoch 85/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 27ms/step - accuracy: 0.5731 - loss: 1.8687\n",
      "Epoch 86/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5822 - loss: 1.7987\n",
      "Epoch 87/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5809 - loss: 1.8136\n",
      "Epoch 88/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 28ms/step - accuracy: 0.5753 - loss: 1.8498\n",
      "Epoch 89/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5622 - loss: 1.9554\n",
      "Epoch 90/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5653 - loss: 1.9513\n",
      "Epoch 91/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5645 - loss: 1.8803\n",
      "Epoch 92/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 27ms/step - accuracy: 0.5710 - loss: 1.8772\n",
      "Epoch 93/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5684 - loss: 1.8871\n",
      "Epoch 94/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5728 - loss: 1.8815\n",
      "Epoch 95/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5745 - loss: 1.8698\n",
      "Epoch 96/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5719 - loss: 1.8608\n",
      "Epoch 97/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 27ms/step - accuracy: 0.5718 - loss: 1.8911\n",
      "Epoch 98/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 57ms/step - accuracy: 0.5679 - loss: 1.8940\n",
      "Epoch 99/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m426s\u001b[0m 339ms/step - accuracy: 0.5594 - loss: 1.9680\n",
      "Epoch 100/100\n",
      "\u001b[1m1257/1257\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 25ms/step - accuracy: 0.5658 - loss: 1.9272\n",
      "<Sequential name=sequential_1, built=True>\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(total_words, 100, input_shape=(max_sequence_len-1,)))\n",
    "model.add(Bidirectional(LSTM(150)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "adam = Adam(learning_rate=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
    "history = model.fit(xs, ys, epochs=100)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf1e17-66a0-4cbc-bce1-512ee142717d",
   "metadata": {},
   "source": [
    "# Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f70ec236-d14f-479b-88a2-45c17571ac6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('MJ_pop_poetry.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4eab70f-e8f6-46ec-97d9-b3ef58825f52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">19</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">327,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">301,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3274</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">985,474</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m19\u001b[0m, \u001b[38;5;34m100\u001b[0m)        │       \u001b[38;5;34m327,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)            │       \u001b[38;5;34m301,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3274\u001b[0m)           │       \u001b[38;5;34m985,474\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,842,224</span> (18.47 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,842,224\u001b[0m (18.47 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,614,074</span> (6.16 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,614,074\u001b[0m (6.16 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,228,150</span> (12.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,228,150\u001b[0m (12.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "#model.save('MJ2_pop_poetry_model.keras')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b902e920-33ca-40b1-a52e-a1ed52837c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flower summer's gotten away from us but around the time le da da da dum mumble oh oh god before scarred war before appeared du someone stalking yourself sunbeams apart flap doing o'neal ferris aa aa aa aa aa tambien actual power him ohms apart 'cause honest hollywood mean 12x apart\n"
     ]
    }
   ],
   "source": [
    "def generate_lyrics(seed_text, next_words=50, model=model, max_sequence_len=max_sequence_len):\n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "\n",
    "print(generate_lyrics(\"flower\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0b17fb3c-f24b-47f8-bf2a-1f513ca62db4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I had a dream of another tomorrow of life and night i'll find day years may side right just ridiculed or fast falls land though falls real son wings him chiller gana plan pure cola hollywood across remedy high nigh high nigh high nigh high chiller gloom stalking mine baby tomorrow other's nowhere tambien\n"
     ]
    }
   ],
   "source": [
    "print(generate_lyrics(\"I had a dream\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "6824d759-071d-4653-afe5-7b9fcb697458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Spring here with you\n",
      " but it feel\n",
      " i was around\n",
      " yeah yeah yeah\n",
      " yeah yeah yeah\n",
      " yeah yeah here\n",
      " worried feed eliminate\n",
      " leaving evening moonwalk\n",
      " millionaire it’s deal\n",
      " or estas alright\n",
      " oak tree tops\n",
      " funny top dear\n",
      " lovely boat cruise\n",
      " heaven\n"
     ]
    }
   ],
   "source": [
    "def generate_pop_poetry(seed_text, model, tokenizer, max_sequence_len, next_words=50, words_per_line=5):\n",
    "    output_text = seed_text\n",
    "    word_count = 0\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        token_list = tokenizer.texts_to_sequences([output_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted_probs = model.predict(token_list, verbose=0)\n",
    "        predicted_index = np.argmax(predicted_probs, axis=-1)[0]\n",
    "        output_word = \"\"\n",
    "        \n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        \n",
    "        output_text += \" \" + output_word\n",
    "        word_count += 1\n",
    "        \n",
    "        if word_count % words_per_line == 0:\n",
    "            output_text += \"\\n\"\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "print(generate_pop_poetry(\n",
    "    seed_text=\"\\n Spring\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_sequence_len=max_sequence_len, \n",
    "    next_words=40, \n",
    "    words_per_line=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "490c0e55-5a2e-4112-bd5f-b0929281eeea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " World i still remember the face\n",
      " her world a kiss you\n",
      " found your side so cold\n",
      " so cold cold fin oak\n",
      " tree tops que je yourself\n",
      " addition by myself strong in\n",
      " stalking singing to rise above\n",
      " tomorrow stalking him selfish terrifying\n",
      " theme pushed right revenge somehow\n",
      " actual plan fin l bien\n",
      " de bois tristeza ms inside\n",
      " c doo doo oooooooh gana\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generate_pop_poetry(\n",
    "    seed_text=\"\\n World\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_sequence_len=max_sequence_len, \n",
    "    next_words=60, \n",
    "    words_per_line=5\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283c712a-fe07-4ace-a952-23c6331005d2",
   "metadata": {},
   "source": [
    "# LAB 2 - Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65206b4e-2b88-41f6-8184-66cd30282090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import re\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c343912-51a7-4289-8690-90d8d0a1c1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lyrics/michael-jackson.txt', 'r', encoding='utf-8') as file:\n",
    "    words = file.read().lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f8ee1005-331f-4c9c-b6bc-dd951d8c2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b2e137a8-e914-4e57-a7c8-ad57731fbea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_model = {}\n",
    "\n",
    "for i in range(len(words)-1):\n",
    "    word = words[i]\n",
    "    next_word = words[i + 1]\n",
    "    if word not in markov_model:\n",
    "        markov_model[word] = []\n",
    "    markov_model[word].append(next_word)\n",
    "\n",
    "#print(markov_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "259c9806-ddac-4c03-8c8b-a51e692d34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spring every day and all the prolems that is breaking why don't stop trippin' why i could be startin' somethin' you and singing to do we fell trapped in vain her about it) (aahh, she dreams tell you want more step (cause we're bring your apartment) dang gone it's got\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_word, length):\n",
    "    current_word = start_word\n",
    "    text = [current_word]\n",
    "    for _ in range(length - 1):\n",
    "        current_word = random.choice(model.get(current_word, ['']))\n",
    "        text.append(current_word)\n",
    "    return ' '.join(text)\n",
    "\n",
    "start_word = \"spring\" \n",
    "generated_text = generate_text(markov_model, start_word, 50)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "773c4127-3bcf-46ac-bc6d-77ead21faf96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spring every hot island if the pain on\n",
      " me a vegetable still i told you g'on\n",
      " get up your every night you don't you\n",
      " babe, hee! hee! hoo! hoo! dancin'-hee! doggone girl\n",
      " i just because it's the soul want you,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_markov_poetry(start_word, model, next_words=50, words_per_line=5):\n",
    "    if start_word not in model:\n",
    "        return \"The start word is not in the model.\"\n",
    "    \n",
    "    output_text = start_word\n",
    "    current_word = start_word\n",
    "    word_count = 1 \n",
    "    \n",
    "    for _ in range(next_words - 1):  \n",
    "        next_words_list = model.get(current_word, [''])\n",
    "        if not next_words_list:  \n",
    "            break\n",
    "        current_word = random.choice(next_words_list)\n",
    "        output_text += \" \" + current_word\n",
    "        word_count += 1\n",
    "        \n",
    "        if word_count % words_per_line == 0:\n",
    "            output_text += \"\\n\"\n",
    "    \n",
    "    return output_text\n",
    "\n",
    "\n",
    "print(generate_markov_poetry(\n",
    "    start_word=\"spring\", \n",
    "    model=markov_model, \n",
    "    next_words=40, \n",
    "    words_per_line=8  \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1845535e-eb51-4373-918a-4af668f98a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_model_2 = defaultdict(lambda: defaultdict(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2bd3679-61b1-48fa-9a2b-e6234f71d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(words) - 1):\n",
    "        current_word = words[i]\n",
    "        next_word = words[i + 1]\n",
    "        markov_model_2[current_word][next_word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26930090-ddb0-41b8-b9cc-65f9427d6501",
   "metadata": {},
   "outputs": [],
   "source": [
    "for current_word, next_words in markov_model_2.items():\n",
    "        total_occurrences = sum(next_words.values())\n",
    "        for next_word in next_words:\n",
    "            next_words[next_word] /= total_occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "04c9e7d9-635f-4f44-bad3-8774ab55aa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why do baby your eyes are kissin' me, sue me how it for us all the change) hoo! i have a torch will be there, you know you said yeah shoo-hee woh woh woh hee! shoo-hee oooh foolish trickery and how lust for us we have been my hand) 'cause you got the way that girl at yourself and i am the lost my baby (yeah, yeah) too high speed, feedback, dolby release two places at the devil anything anything anything anything anything for me with me!) i'm telling you make it (bad bad - you on time around comes\n"
     ]
    }
   ],
   "source": [
    "lyric_length = 100\n",
    "punctuation_marks = ['.', ',', ';', ':', '!', '?']\n",
    "\n",
    "lyric = [current_word]\n",
    "\n",
    "for _ in range(lyric_length - 1):\n",
    "    if current_word in markov_model_2:\n",
    "        next_words = markov_model_2[current_word]\n",
    "        \n",
    "        # Choose the next word based on its probability\n",
    "        next_word = random.choices(list(next_words.keys()), weights=list(next_words.values()), k=1)[0]\n",
    "        \n",
    "        # Append the next word to the lyric\n",
    "        if next_word in punctuation_marks:\n",
    "            lyric.append(next_word)  # Add punctuation directly\n",
    "        else:\n",
    "            lyric.append(' ' + next_word)  # Add space before non-punctuation words\n",
    "        \n",
    "        # Update current_word for the next iteration\n",
    "        current_word = next_word\n",
    "    else:\n",
    "        # If the current word has no next words, end the lyric\n",
    "        break\n",
    "\n",
    "# Generate the lyric\n",
    "current_word = \"why\"\n",
    "lyric_str = ''.join(lyric)\n",
    "print(lyric_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6f28f9ed-4153-4ec5-b697-035548bce037",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poem_style(word_list):\n",
    "    for i in range(0, (lyric_length - 1), 15):\n",
    "    \n",
    "        lines = [\n",
    "        \" \".join(word_list[i:i+5]),\n",
    "        \" \".join(word_list[i+4:i+10]),\n",
    "        \" \".join(word_list[i+10:i+15]),\n",
    "        \" \".join(word_list[i+15:i+20]),\n",
    "        \" \".join(word_list[i+20:i+25])\n",
    "        ]\n",
    "\n",
    "        # Print the poem\n",
    "        for line in lines:\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ab108966-ebbb-4670-98e9-31ab44112bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "why  do  baby  your  eyes\n",
      " eyes  are  kissin'  me,  sue  me\n",
      " how  it  for  us  all\n",
      " the  change)  hoo!  i  have\n",
      " a  torch  will  be  there,\n",
      " the  change)  hoo!  i  have\n",
      " have  a  torch  will  be  there,\n",
      " you  know  you  said  yeah\n",
      " shoo-hee  woh  woh  woh  hee!\n",
      " shoo-hee  oooh  foolish  trickery  and\n",
      " shoo-hee  woh  woh  woh  hee!\n",
      " hee!  shoo-hee  oooh  foolish  trickery  and\n",
      " how  lust  for  us  we\n",
      " have  been  my  hand)  'cause\n",
      " you  got  the  way  that\n",
      " have  been  my  hand)  'cause\n",
      " 'cause  you  got  the  way  that\n",
      " girl  at  yourself  and  i\n",
      " am  the  lost  my  baby\n",
      " (yeah,  yeah)  too  high  speed,\n",
      " am  the  lost  my  baby\n",
      " baby  (yeah,  yeah)  too  high  speed,\n",
      " feedback,  dolby  release  two  places\n",
      " at  the  devil  anything  anything\n",
      " anything  anything  anything  for  me\n",
      " at  the  devil  anything  anything\n",
      " anything  anything  anything  anything  for  me\n",
      " with  me!)  i'm  telling  you\n",
      " make  it  (bad  bad  -\n",
      " you  on  time  around  comes\n",
      " make  it  (bad  bad  -\n",
      " -  you  on  time  around  comes\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poem_style(lyric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c7973-1a5b-4548-a06f-68b7cf1f7930",
   "metadata": {},
   "source": [
    "# Reload the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bba0e89f-6eb1-4ecb-bccd-e60214588e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0ad2f8d0-02c7-449b-9967-f925cbdec364",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A total of 1 objects could not be loaded. Example error message for object <LSTMCell name=lstm_cell, built=True>:\n\nLayer 'lstm_cell' expected 3 variables, but received 0 variables during loading. Expected: ['kernel', 'recurrent_kernel', 'bias']\n\nList of objects that could not be loaded:\n[<LSTMCell name=lstm_cell, built=True>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMJ_pop_poetry.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_api.py:176\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    173\u001b[0m         is_keras_zip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip:\n\u001b[0;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    177\u001b[0m         filepath,\n\u001b[1;32m    178\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    180\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    181\u001b[0m     )\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.hdf5\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(filepath)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:192\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[1;32m    189\u001b[0m         asset_store\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m failed_trackables:\n\u001b[0;32m--> 192\u001b[0m         _raise_loading_failure(error_msgs)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:273\u001b[0m, in \u001b[0;36m_raise_loading_failure\u001b[0;34m(error_msgs, warn_only)\u001b[0m\n\u001b[1;32m    271\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[0;31mValueError\u001b[0m: A total of 1 objects could not be loaded. Example error message for object <LSTMCell name=lstm_cell, built=True>:\n\nLayer 'lstm_cell' expected 3 variables, but received 0 variables during loading. Expected: ['kernel', 'recurrent_kernel', 'bias']\n\nList of objects that could not be loaded:\n[<LSTMCell name=lstm_cell, built=True>]"
     ]
    }
   ],
   "source": [
    "new_model = load_model('MJ_pop_poetry.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460bc9cf-33e9-44eb-a9c8-b7f5778d75c9",
   "metadata": {},
   "source": [
    "note: somehow I failed at reloading mt RNN kera model, which needs to be further investgated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276875e-9f88-43eb-8069-765afd717d97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
